{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy data\n",
    "def create_dummy_data(n_samples = 100, n_features = 3, n_informative = 3, n_redundant = 0, random_state = 42):\n",
    "\n",
    "    # Create the dataset\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_informative=n_informative,\n",
    "        n_redundant=n_redundant,\n",
    "        n_classes=2,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Convert to DataFrame for convenience\n",
    "    df = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(n_features)])\n",
    "    df['label'] = y\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_dummy_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.055047</td>\n",
       "      <td>-0.076900</td>\n",
       "      <td>1.014236</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.539644</td>\n",
       "      <td>-1.135646</td>\n",
       "      <td>0.951151</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.801402</td>\n",
       "      <td>0.114717</td>\n",
       "      <td>-0.534702</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.203781</td>\n",
       "      <td>-2.440252</td>\n",
       "      <td>2.165799</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.766340</td>\n",
       "      <td>0.198916</td>\n",
       "      <td>-1.456762</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2.510895</td>\n",
       "      <td>-1.187769</td>\n",
       "      <td>-1.737078</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.916990</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>1.309143</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.924313</td>\n",
       "      <td>-1.563592</td>\n",
       "      <td>0.495242</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.650580</td>\n",
       "      <td>-1.108798</td>\n",
       "      <td>-0.227124</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.813396</td>\n",
       "      <td>2.378734</td>\n",
       "      <td>1.097880</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    feature_0  feature_1  feature_2  label\n",
       "0   -0.055047  -0.076900   1.014236      1\n",
       "1   -1.539644  -1.135646   0.951151      0\n",
       "2   -0.801402   0.114717  -0.534702      0\n",
       "3    2.203781  -2.440252   2.165799      1\n",
       "4   -0.766340   0.198916  -1.456762      0\n",
       "..        ...        ...        ...    ...\n",
       "95   2.510895  -1.187769  -1.737078      0\n",
       "96  -0.916990   0.062745   1.309143      0\n",
       "97   1.924313  -1.563592   0.495242      0\n",
       "98   1.650580  -1.108798  -0.227124      0\n",
       "99   0.813396   2.378734   1.097880      1\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.82%\n"
     ]
    }
   ],
   "source": [
    "# split data into train and test sets\n",
    "X = data.drop('label', axis=1)\n",
    "y = data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# fit model no training data\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete\n",
      "Accuracy: 90.91%\n"
     ]
    }
   ],
   "source": [
    "from customxgboost import XGBoostClassifier as myxgb\n",
    "\n",
    "my_model = myxgb()\n",
    "my_model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for test data\n",
    "y_pred = my_model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "Training Complete\n",
      "Accuracy: 81.82%\n"
     ]
    }
   ],
   "source": [
    "my_model = myxgb(method='hist')\n",
    "my_model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for test data\n",
    "y_pred = my_model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Histogram:\n",
    "    def __init__(self, max_bin=256):\n",
    "        self.hist_data = {}\n",
    "        self.max_bin = max_bin\n",
    "\n",
    "    def create_histogram(self, data):\n",
    "        \"\"\"\n",
    "        Create quantile-based histograms for each feature in the dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - data: DataFrame containing features.\n",
    "        \"\"\"\n",
    "        for column in data.columns:\n",
    "            # Compute quantile-based bins\n",
    "            bin_edges = np.quantile(data[column], q=np.linspace(0, 1, self.max_bin + 1))\n",
    "            values, _ = np.histogram(data[column], bins=bin_edges)\n",
    "\n",
    "            self.hist_data[column] = {\n",
    "                'values': values,\n",
    "                'bin_edges': bin_edges\n",
    "            }\n",
    "\n",
    "    def display_histogram(self, feature_name):\n",
    "        \"\"\"\n",
    "        Display the histogram for a specific feature.\n",
    "\n",
    "        Parameters:\n",
    "        - feature_name: The name of the feature to display.\n",
    "        \"\"\"\n",
    "        if feature_name not in self.hist_data:\n",
    "            print(f\"Feature '{feature_name}' does not exist in the histogram data.\")\n",
    "            return\n",
    "\n",
    "        values = self.hist_data[feature_name]['values']\n",
    "        bin_edges = self.hist_data[feature_name]['bin_edges']\n",
    "\n",
    "        plt.bar(bin_edges[:-1], values, width=np.diff(bin_edges), edgecolor=\"black\", align=\"edge\")\n",
    "        plt.title(f\"Quantile-Based Histogram of {feature_name}\")\n",
    "        plt.xlabel(feature_name)\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import e\n",
    "\n",
    "class Node:\n",
    "    \n",
    "    '''\n",
    "    A node object that is recursivly called within itslef to construct a regression tree. Based on Tianqi Chen's XGBoost \n",
    "    the internal gain used to find the optimal split value uses both the gradient and hessian. Also a weighted quantlie sketch \n",
    "    and optimal leaf values all follow Chen's description in \"XGBoost: A Scalable Tree Boosting System\" the only thing not \n",
    "    implemented in this version is sparsity aware fitting or the ability to handle NA values with a default direction.\n",
    "\n",
    "    Inputs\n",
    "    ------------------------------------------------------------------------------------------------------------------\n",
    "    x: pandas datframe of the training data\n",
    "    gradient: negative gradient of the loss function\n",
    "    hessian: second order derivative of the loss function\n",
    "    idxs: used to keep track of samples within the tree structure\n",
    "    subsample_cols: is an implementation of layerwise column subsample randomizing the structure of the trees\n",
    "    (complexity parameter)\n",
    "    min_leaf: minimum number of samples for a node to be considered a node (complexity parameter)\n",
    "    min_child_weight: sum of the heassian inside a node is a meaure of purity (complexity parameter)\n",
    "    depth: limits the number of layers in the tree\n",
    "    lambda: L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "    gamma: This parameter also prevents over fitting and is present in the the calculation of the gain (structure score). \n",
    "    As this is subtracted from the gain it essentially sets a minimum gain amount to make a split in a node.\n",
    "    eps: This parameter is used in the quantile weighted skecth or 'approx' tree method roughly translates to \n",
    "    (1 / sketch_eps) number of bins\n",
    "\n",
    "    Outputs\n",
    "    --------------------------------------------------------------------------------------------------------------------\n",
    "    A single tree object that will be used for gradient boosintg.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, x, gradient, hessian, idxs, subsample_cols = 0.8 , min_leaf = 5, min_child_weight = 1 ,depth = 10, lambda_ = 1, gamma = 1, eps = 0.1):\n",
    "      \n",
    "        self.x, self.gradient, self.hessian = x, gradient, hessian\n",
    "        self.idxs = idxs \n",
    "        self.depth = depth\n",
    "        self.min_leaf = min_leaf\n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma  = gamma\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.row_count = len(idxs)\n",
    "        self.col_count = x.shape[1]\n",
    "        self.subsample_cols = subsample_cols\n",
    "        self.eps = eps\n",
    "        self.column_subsample = np.random.permutation(self.col_count)[:round(self.subsample_cols*self.col_count)]\n",
    "        \n",
    "        self.val = self.compute_gamma(self.gradient[self.idxs], self.hessian[self.idxs])\n",
    "          \n",
    "        self.score = float('-inf')\n",
    "        self.find_varsplit()\n",
    "        \n",
    "        \n",
    "    def compute_gamma(self, gradient, hessian):\n",
    "        '''\n",
    "        Calculates the optimal leaf value equation (5) in \"XGBoost: A Scalable Tree Boosting System\"\n",
    "        '''\n",
    "        return(-np.sum(gradient)/(np.sum(hessian) + self.lambda_))\n",
    "        \n",
    "    def find_varsplit(self):\n",
    "        '''\n",
    "        Scans through every column and calcuates the best split point.\n",
    "        The node is then split at this point and two new nodes are created.\n",
    "        Depth is only parameter to change as we have added a new layer to tre structure.\n",
    "        If no split is better than the score initalised at the begining then no splits further splits are made\n",
    "        '''\n",
    "        for c in self.column_subsample: self.find_greedy_split(c)\n",
    "        if self.is_leaf: return\n",
    "        x = self.split_col\n",
    "        lhs = np.nonzero(x <= self.split)[0]\n",
    "        rhs = np.nonzero(x > self.split)[0]\n",
    "        self.lhs = Node(x = self.x, gradient = self.gradient, hessian = self.hessian, idxs = self.idxs[lhs], min_leaf = self.min_leaf, depth = self.depth-1, lambda_ = self.lambda_ , gamma = self.gamma, min_child_weight = self.min_child_weight, eps = self.eps, subsample_cols = self.subsample_cols)\n",
    "        self.rhs = Node(x = self.x, gradient = self.gradient, hessian = self.hessian, idxs = self.idxs[rhs], min_leaf = self.min_leaf, depth = self.depth-1, lambda_ = self.lambda_ , gamma = self.gamma, min_child_weight = self.min_child_weight, eps = self.eps, subsample_cols = self.subsample_cols)\n",
    "        \n",
    "    def find_greedy_split(self, var_idx):\n",
    "        '''\n",
    "         For a given feature greedily calculates the gain at each split.\n",
    "         Globally updates the best score and split point if a better split point is found\n",
    "        '''\n",
    "        x = self.x[self.idxs, var_idx]\n",
    "        \n",
    "        for r in range(self.row_count):\n",
    "            lhs = x <= x[r]\n",
    "            rhs = x > x[r]\n",
    "            \n",
    "            lhs_indices = np.nonzero(x <= x[r])[0]\n",
    "            rhs_indices = np.nonzero(x > x[r])[0]\n",
    "            if(rhs.sum() < self.min_leaf or lhs.sum() < self.min_leaf \n",
    "               or self.hessian[lhs_indices].sum() < self.min_child_weight\n",
    "               or self.hessian[rhs_indices].sum() < self.min_child_weight): continue\n",
    "\n",
    "            curr_score = self.gain(lhs, rhs)\n",
    "            if curr_score > self.score: \n",
    "                self.var_idx = var_idx\n",
    "                self.score = curr_score\n",
    "                self.split = x[r]\n",
    "                \n",
    "    def weighted_qauntile_sketch(self, var_idx):\n",
    "        '''\n",
    "        XGBOOST Mini-Version\n",
    "        Yiyang \"Joe\" Zeng\n",
    "        Is an approximation to the eact greedy approach faster for bigger datasets wher it is not feasible\n",
    "        to calculate the gain at every split point. Uses equation (8) and (9) from \"XGBoost: A Scalable Tree Boosting System\"\n",
    "        '''\n",
    "        x = self.x[self.idxs, var_idx]\n",
    "        hessian_ = self.hessian[self.idxs]\n",
    "        df = pd.DataFrame({'feature':x,'hess':hessian_})\n",
    "        \n",
    "        df.sort_values(by=['feature'], ascending = True, inplace = True)\n",
    "        hess_sum = df['hess'].sum() \n",
    "        df['rank'] = df.apply(lambda x : (1/hess_sum)*sum(df[df['feature'] < x['feature']]['hess']), axis=1)\n",
    "        \n",
    "        for row in range(df.shape[0]-1):\n",
    "            # look at the current rank and the next ran\n",
    "            rk_sk_j, rk_sk_j_1 = df['rank'].iloc[row:row+2]\n",
    "            diff = abs(rk_sk_j - rk_sk_j_1)\n",
    "            if(diff >= self.eps):\n",
    "                continue\n",
    "                \n",
    "            split_value = (df['rank'].iloc[row+1] + df['rank'].iloc[row])/2\n",
    "            lhs = x <= split_value\n",
    "            rhs = x > split_value\n",
    "            \n",
    "            lhs_indices = np.nonzero(x <= split_value)[0]\n",
    "            rhs_indices = np.nonzero(x > split_value)[0]\n",
    "            if(rhs.sum() < self.min_leaf or lhs.sum() < self.min_leaf \n",
    "               or self.hessian[lhs_indices].sum() < self.min_child_weight\n",
    "               or self.hessian[rhs_indices].sum() < self.min_child_weight): continue\n",
    "                \n",
    "            curr_score = self.gain(lhs, rhs)\n",
    "            if curr_score > self.score: \n",
    "                self.var_idx = var_idx\n",
    "                self.score = curr_score\n",
    "                self.split = split_value\n",
    "                \n",
    "    def gain(self, lhs, rhs):\n",
    "        '''\n",
    "        Calculates the gain at a particular split point bases on equation (7) from\n",
    "        \"XGBoost: A Scalable Tree Boosting System\"\n",
    "        '''\n",
    "        gradient = self.gradient[self.idxs]\n",
    "        hessian  = self.hessian[self.idxs]\n",
    "        \n",
    "        lhs_gradient = gradient[lhs].sum()\n",
    "        lhs_hessian  = hessian[lhs].sum()\n",
    "        \n",
    "        rhs_gradient = gradient[rhs].sum()\n",
    "        rhs_hessian  = hessian[rhs].sum()\n",
    "        \n",
    "        gain = 0.5 *( (lhs_gradient**2/(lhs_hessian + self.lambda_)) + (rhs_gradient**2/(rhs_hessian + self.lambda_)) - ((lhs_gradient + rhs_gradient)**2/(lhs_hessian + rhs_hessian + self.lambda_))) - self.gamma\n",
    "        return(gain)\n",
    "                \n",
    "    @property\n",
    "    def split_col(self):\n",
    "        '''\n",
    "        splits a column \n",
    "        '''\n",
    "        return self.x[self.idxs , self.var_idx]\n",
    "                \n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        '''\n",
    "        checks if node is a leaf\n",
    "        '''\n",
    "        return self.score == float('-inf') or self.depth <= 0                 \n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.array([self.predict_row(xi) for xi in x])\n",
    "    \n",
    "    def predict_row(self, xi):\n",
    "        if self.is_leaf:\n",
    "            return(self.val)\n",
    "\n",
    "        node = self.lhs if xi[self.var_idx] <= self.split else self.rhs\n",
    "        return node.predict_row(xi)\n",
    "    \n",
    "\n",
    "class HistogramNode:\n",
    "    def __init__(self, hist_data, feature_names, depth=10, min_leaf=5, min_child_weight=1, lambda_=1, gamma=1):\n",
    "        \"\"\"\n",
    "        Initializes a tree node using histogram data for XGBoost-style training.\n",
    "        \n",
    "        Parameters:\n",
    "        - hist_data: Precomputed histogram data containing 'values', 'gradient', 'hessian', and 'bin_edges' for each feature.\n",
    "        - feature_names: List of feature names corresponding to the hist_data.\n",
    "        - depth: Maximum depth of the tree.\n",
    "        - min_leaf: Minimum number of samples required in a leaf node.\n",
    "        - min_child_weight: Minimum sum of Hessians required for a split.\n",
    "        - lambda_: Regularization parameter for leaf weights.\n",
    "        - gamma: Minimum gain required to make a split.\n",
    "        \"\"\"\n",
    "        self.hist_data = hist_data\n",
    "        self.feature_names = feature_names\n",
    "        self.depth = depth\n",
    "        self.min_leaf = min_leaf\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Node-specific properties\n",
    "        self.score = float('-inf')\n",
    "        self.split_feature = None\n",
    "        self.split_value = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.val = self.compute_leaf_value()\n",
    "\n",
    "        # Build the tree\n",
    "        self.find_best_split()\n",
    "\n",
    "    def compute_leaf_value(self):\n",
    "        \"\"\"\n",
    "        Computes the optimal value for the current node's leaf.\n",
    "        \"\"\"\n",
    "        total_grad = sum(hist['gradient'].sum() for hist in self.hist_data.values())\n",
    "        total_hess = sum(hist['hessian'].sum() for hist in self.hist_data.values())\n",
    "        return -total_grad / (total_hess + self.lambda_)\n",
    "\n",
    "    def find_best_split(self):\n",
    "        \"\"\"\n",
    "        Finds the best split for the current node using the histogram data.\n",
    "        \"\"\"\n",
    "        if self.depth <= 0:\n",
    "            return\n",
    "\n",
    "        for feature in self.feature_names:\n",
    "            hist = self.hist_data[feature]\n",
    "            gradients = hist['gradient']\n",
    "            hessians = hist['hessian']\n",
    "            values = hist['values']\n",
    "            bin_edges = hist['bin_edges']\n",
    "\n",
    "            for bin_idx in range(len(bin_edges) - 1):\n",
    "                lhs_hess = hessians[:bin_idx + 1].sum()\n",
    "                rhs_hess = hessians[bin_idx + 1:].sum()\n",
    "\n",
    "                if lhs_hess < self.min_child_weight or rhs_hess < self.min_child_weight:\n",
    "                    continue\n",
    "\n",
    "                lhs_grad = gradients[:bin_idx + 1].sum()\n",
    "                rhs_grad = gradients[bin_idx + 1:].sum()\n",
    "\n",
    "                gain = 0.5 * (\n",
    "                    (lhs_grad ** 2 / (lhs_hess + self.lambda_))\n",
    "                    + (rhs_grad ** 2 / (rhs_hess + self.lambda_))\n",
    "                    - ((lhs_grad + rhs_grad) ** 2 / (lhs_hess + rhs_hess + self.lambda_))\n",
    "                ) - self.gamma\n",
    "\n",
    "                if gain > self.score:\n",
    "                    self.score = gain\n",
    "                    self.split_feature = feature\n",
    "                    self.split_value = (bin_edges[bin_idx] + bin_edges[bin_idx + 1]) / 2\n",
    "\n",
    "        # Stop splitting if no valid split is found\n",
    "        if self.score == float('-inf'):\n",
    "            return\n",
    "\n",
    "        self.split_node()\n",
    "\n",
    "    def split_node(self):\n",
    "        \"\"\"\n",
    "        Splits the current node into left and right children.\n",
    "        \"\"\"\n",
    "        if self.depth <= 0 or self.score == float('-inf'):\n",
    "            return\n",
    "\n",
    "        left_hist_data = {}\n",
    "        right_hist_data = {}\n",
    "\n",
    "        for feature in self.feature_names:\n",
    "            hist = self.hist_data[feature]\n",
    "            bin_edges = hist['bin_edges']\n",
    "            split_idx = np.searchsorted(bin_edges, self.split_value, side=\"right\") - 1\n",
    "\n",
    "            left_hist_data[feature] = {\n",
    "                'values': hist['values'][:split_idx + 1],\n",
    "                'gradient': hist['gradient'][:split_idx + 1],\n",
    "                'hessian': hist['hessian'][:split_idx + 1],\n",
    "                'bin_edges': bin_edges[:split_idx + 2],\n",
    "            }\n",
    "\n",
    "            right_hist_data[feature] = {\n",
    "                'values': hist['values'][split_idx + 1:],\n",
    "                'gradient': hist['gradient'][split_idx + 1:],\n",
    "                'hessian': hist['hessian'][split_idx + 1:],\n",
    "                'bin_edges': bin_edges[split_idx + 1:],\n",
    "            }\n",
    "\n",
    "        self.left = HistogramNode(\n",
    "            left_hist_data, self.feature_names, self.depth - 1, self.min_leaf, self.min_child_weight, self.lambda_, self.gamma\n",
    "        )\n",
    "        self.right = HistogramNode(\n",
    "            right_hist_data, self.feature_names, self.depth - 1, self.min_leaf, self.min_child_weight, self.lambda_, self.gamma\n",
    "        )\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the output for a DataFrame of input instances.\n",
    "        \"\"\"\n",
    "        return np.array([self.predict_row(xi) for _, xi in X.iterrows()])  # Iterating through DataFrame rows\n",
    "\n",
    "    def predict_row(self, xi):\n",
    "        \"\"\"\n",
    "        Predicts a single instance based on the tree structure.\n",
    "        \"\"\"\n",
    "        if self.left is None or self.right is None:\n",
    "            return self.val\n",
    "\n",
    "        # Ensure xi is a pandas Series (it should be when iterating with DataFrame.iterrows())\n",
    "        if xi[self.split_feature] <= self.split_value:\n",
    "            return self.left.predict_row(xi)\n",
    "        else:\n",
    "            return self.right.predict_row(xi)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class XGBoostTree:\n",
    "    '''\n",
    "    Wrapper class that provides a scikit learn interface to the recursive regression tree above\n",
    "    \n",
    "    Inputs\n",
    "    ------------------------------------------------------------------------------------------------------------------\n",
    "    x: pandas datframe of the training data\n",
    "    gradient: negative gradient of the loss function\n",
    "    hessian: second order derivative of the loss function\n",
    "    idxs: used to keep track of samples within the tree structure\n",
    "    subsample_cols: is an implementation of layerwise column subsample randomizing the structure of the trees\n",
    "    (complexity parameter)\n",
    "    min_leaf: minimum number of samples for a node to be considered a node (complexity parameter)\n",
    "    min_child_weight: sum of the heassian inside a node is a meaure of purity (complexity parameter)\n",
    "    depth: limits the number of layers in the tree\n",
    "    lambda: L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "    gamma: This parameter also prevents over fitting and is present in the the calculation of the gain (structure score). \n",
    "    As this is subtracted from the gain it essentially sets a minimum gain amount to make a split in a node.\n",
    "    eps: This parameter is used in the quantile weighted skecth or 'approx' tree method roughly translates to \n",
    "    (1 / sketch_eps) number of bins\n",
    "    \n",
    "    Outputs\n",
    "    --------------------------------------------------------------------------------------------------------------------\n",
    "    A single tree object that will be used for gradient boosintg.\n",
    "    \n",
    "    '''\n",
    "    def fit(self, x, gradient, hessian, subsample_cols = 0.8 , min_leaf = 5, min_child_weight = 1 ,depth = 10, lambda_ = 1, gamma = 1, eps = 0.1):\n",
    "        self.dtree = Node(x, gradient, hessian, np.array(np.arange(len(x))), subsample_cols, min_leaf, min_child_weight, depth, lambda_, gamma, eps)\n",
    "        return self\n",
    "    \n",
    "    def hist_fit(self, histograms, subsample_cols=0.8, min_leaf=5, min_child_weight=1, depth=10, lambda_=1, gamma=1, eps=0.1):\n",
    "        \"\"\"\n",
    "        Fits a regression tree using precomputed histograms for XGBoost-style training.\n",
    "\n",
    "        Parameters:\n",
    "        - histograms: A dictionary where each key is a feature name, and the value is a dictionary containing\n",
    "                    'values', 'gradient', 'hessian', and 'bin_edges' for that feature.\n",
    "        - subsample_cols: Fraction of features to randomly sample for splits at each level.\n",
    "        - min_leaf: Minimum number of samples required in a leaf node.\n",
    "        - min_child_weight: Minimum sum of Hessians required for a split.\n",
    "        - depth: Maximum depth of the tree.\n",
    "        - lambda_: Regularization parameter for leaf weights.\n",
    "        - gamma: Minimum gain required to make a split.\n",
    "        - eps: Used for quantile sketching (not needed for histograms but kept for interface consistency).\n",
    "\n",
    "        Returns:\n",
    "        - self: The fitted tree.\n",
    "        \"\"\"\n",
    "        # Subsample columns for each level of the tree\n",
    "        feature_names = list(histograms.keys())\n",
    "        num_features = len(feature_names)\n",
    "        num_selected_features = max(1, int(subsample_cols * num_features))\n",
    "        selected_features = np.random.choice(feature_names, num_selected_features, replace=False)\n",
    "\n",
    "        # Filter histograms to only include selected features\n",
    "        selected_histograms = {feature: histograms[feature] for feature in selected_features}\n",
    "\n",
    "        # Build the tree using HistogramNode\n",
    "        self.dtree = HistogramNode(\n",
    "            hist_data=selected_histograms,\n",
    "            feature_names=selected_features,\n",
    "            depth=depth,\n",
    "            min_leaf=min_leaf,\n",
    "            min_child_weight=min_child_weight,\n",
    "            lambda_=lambda_,\n",
    "            gamma=gamma\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.dtree.predict(X)\n",
    "   \n",
    "   \n",
    "class XGBoostClassifier:\n",
    "    '''\n",
    "    Full application of the XGBoost algorithm as described in \"XGBoost: A Scalable Tree Boosting System\" for \n",
    "    Binary Classification.\n",
    "\n",
    "    Inputs\n",
    "    ------------------------------------------------------------------------------------------------------------------\n",
    "    x: pandas datframe of the training data\n",
    "    gradient: negative gradient of the loss function\n",
    "    hessian: second order derivative of the loss function\n",
    "    idxs: used to keep track of samples within the tree structure\n",
    "    subsample_cols: is an implementation of layerwise column subsample randomizing the structure of the trees\n",
    "    (complexity parameter)\n",
    "    min_leaf: minimum number of samples for a node to be considered a node (complexity parameter)\n",
    "    min_child_weight: sum of the heassian inside a node is a meaure of purity (complexity parameter)\n",
    "    depth: limits the number of layers in the tree\n",
    "    lambda: L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "    gamma: This parameter also prevents over fitting and is present in the the calculation of the gain (structure score). \n",
    "    As this is subtracted from the gain it essentially sets a minimum gain amount to make a split in a node.\n",
    "    eps: This parameter is used in the quantile weighted skecth or 'approx' tree method roughly translates to \n",
    "    (1 / sketch_eps) number of bins\n",
    "\n",
    "    Outputs\n",
    "    --------------------------------------------------------------------------------------------------------------------\n",
    "    A single tree object that will be used for gradient boosintg.\n",
    "    '''\n",
    "    def __init__(self, method = 'regular'):\n",
    "        self.estimators = []\n",
    "        self.method = method\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    # first order gradient logLoss\n",
    "    def grad(self, preds, labels):\n",
    "        preds = self.sigmoid(preds)\n",
    "        return(preds - labels)\n",
    "    \n",
    "    # second order gradient logLoss\n",
    "    def hess(self, preds, labels):\n",
    "        preds = self.sigmoid(preds)\n",
    "        return(preds * (1 - preds))\n",
    "    \n",
    "    @staticmethod\n",
    "    def log_odds(column):\n",
    "        binary_yes = np.count_nonzero(column == 1)\n",
    "        binary_no  = np.count_nonzero(column == 0)\n",
    "        return(np.log(binary_yes/binary_no))\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, subsample_cols = 0.8 , min_child_weight = 1, depth = 5, min_leaf = 5, learning_rate = 0.4, boosting_rounds = 5, lambda_ = 1.5, gamma = 1, eps = 0.1):\n",
    "\n",
    "        self.depth = depth\n",
    "        self.subsample_cols = subsample_cols\n",
    "        self.eps = eps\n",
    "        self.min_child_weight = min_child_weight \n",
    "        self.min_leaf = min_leaf\n",
    "        self.learning_rate = learning_rate\n",
    "        self.boosting_rounds = boosting_rounds \n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma  = gamma\n",
    "    \n",
    "        self.base_pred = np.full((X.shape[0], 1), 0).flatten().astype('float64')\n",
    "        \n",
    "        if self.method == 'regular':\n",
    "            X = X.to_numpy() if isinstance(X, pd.DataFrame) else X\n",
    "            y = y.to_numpy() if isinstance(y, pd.Series) else y\n",
    "            self.X, self.y = X, y\n",
    "            for booster in range(self.boosting_rounds):\n",
    "                Grad = self.grad(self.base_pred, self.y)\n",
    "                Hess = self.hess(self.base_pred, self.y)\n",
    "                boosting_tree = XGBoostTree().fit(self.X, Grad, Hess, depth = self.depth, min_leaf = self.min_leaf, lambda_ = self.lambda_, gamma = self.gamma, eps = self.eps, min_child_weight = self.min_child_weight, subsample_cols = self.subsample_cols)\n",
    "                self.base_pred += self.learning_rate * boosting_tree.predict(self.X)\n",
    "                self.estimators.append(boosting_tree)\n",
    "\n",
    "        elif self.method == 'hist':\n",
    "            self.max_bin = 256\n",
    "            self.hist_data = {}\n",
    "            for booster in range(self.boosting_rounds):\n",
    "                self.compute_histograms(X, y.to_numpy(), self.base_pred, max_bins=self.max_bin)\n",
    "                boosting_tree = XGBoostTree().hist_fit(self.hist_data, subsample_cols = self.subsample_cols, min_leaf = self.min_leaf, min_child_weight = self.min_child_weight, depth = self.depth, lambda_ = self.lambda_, gamma = self.gamma, eps = self.eps)\n",
    "                self.base_pred += self.learning_rate * boosting_tree.predict(X)\n",
    "                self.estimators.append(boosting_tree)\n",
    "\n",
    "        print('Training Complete')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compute_histograms(self, X, y, y_preds, max_bins=256):\n",
    "\n",
    "\n",
    "\n",
    "        for column in X.columns:\n",
    "            # Compute quantile-based bins\n",
    "            bin_edges = np.quantile(X[column], q=np.linspace(0, 1, max_bins + 1))\n",
    "            \n",
    "            # Digitize feature values into bins\n",
    "            bin_indices = np.digitize(X[column], bins=bin_edges, right=False) - 1\n",
    "            bin_indices[bin_indices == max_bins] = max_bins - 1  # Handle edge case for right boundary\n",
    "            \n",
    "            # Preallocate arrays for gradients and Hessians\n",
    "            gradients = np.zeros(max_bins)\n",
    "            hessians = np.zeros(max_bins)\n",
    "            \n",
    "            # Aggregate gradients and Hessians for each bin\n",
    "            for bin_idx in range(max_bins):\n",
    "                indices = np.where(bin_indices == bin_idx)[0]\n",
    "                if len(indices) > 0:\n",
    "                    gradients[bin_idx] = np.sum(self.grad(y_preds[indices], y[indices]))\n",
    "                    hessians[bin_idx] = np.sum(self.hess(y_preds[indices], y[indices]))\n",
    "            \n",
    "            # Store results in the histogram data\n",
    "            self.hist_data[column] = {\n",
    "                'values': np.bincount(bin_indices, minlength=max_bins),\n",
    "                'gradient': gradients,\n",
    "                'hessian': hessians,\n",
    "                'bin_edges': bin_edges\n",
    "            }\n",
    "\n",
    "          \n",
    "    def predict_proba(self, X):\n",
    "        pred = np.zeros(X.shape[0])\n",
    "        \n",
    "        for estimator in self.estimators:\n",
    "            pred += self.learning_rate * estimator.predict(X) \n",
    "          \n",
    "        return(self.sigmoid(np.full((X.shape[0], 1), 1).flatten().astype('float64') + pred))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.method == 'regular':\n",
    "            X = X.to_numpy() if isinstance(X, pd.DataFrame) else X\n",
    "        pred = np.zeros(X.shape[0])\n",
    "        for estimator in self.estimators:\n",
    "            pred += self.learning_rate * estimator.predict(X) \n",
    "        \n",
    "        predicted_probas = self.sigmoid(np.full((X.shape[0], 1), 1).flatten().astype('float64') + pred)\n",
    "        preds = np.where(predicted_probas > np.mean(predicted_probas), 1, 0)\n",
    "        return(preds)\n",
    "       \n",
    "       \n",
    "class XGBoostRegressor:\n",
    "    '''\n",
    "    Full application of the XGBoost algorithm as described in \"XGBoost: A Scalable Tree Boosting System\" for \n",
    "    regression.\n",
    "\n",
    "    Inputs\n",
    "    ------------------------------------------------------------------------------------------------------------------\n",
    "    x: pandas datframe of the training data\n",
    "    gradient: negative gradient of the loss function\n",
    "    hessian: second order derivative of the loss function\n",
    "    idxs: used to keep track of samples within the tree structure\n",
    "    subsample_cols: is an implementation of layerwise column subsample randomizing the structure of the trees\n",
    "    (complexity parameter)\n",
    "    min_leaf: minimum number of samples for a node to be considered a node (complexity parameter)\n",
    "    min_child_weight: sum of the heassian inside a node is a meaure of purity (complexity parameter)\n",
    "    depth: limits the number of layers in the tree\n",
    "    lambda: L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "    gamma: This parameter also prevents over fitting and is present in the the calculation of the gain (structure score). \n",
    "    As this is subtracted from the gain it essentially sets a minimum gain amount to make a split in a node.\n",
    "    eps: This parameter is used in the quantile weighted skecth or 'approx' tree method roughly translates to \n",
    "    (1 / sketch_eps) number of bins\n",
    "\n",
    "    Outputs\n",
    "    --------------------------------------------------------------------------------------------------------------------\n",
    "    A single tree object that will be used for gradient boosintg.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.estimators = []\n",
    "    \n",
    "    # first order gradient mean squared error\n",
    "    @staticmethod\n",
    "    def grad(preds, labels):\n",
    "        return(2*(preds-labels))\n",
    "    \n",
    "    # second order gradient logLoss\n",
    "    @staticmethod\n",
    "    def hess(preds, labels):\n",
    "        '''\n",
    "        hessian of mean squared error is a constant value of two \n",
    "        returns an array of twos\n",
    "        '''\n",
    "        return(np.full((preds.shape[0], 1), 2).flatten().astype('float64'))\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, subsample_cols = 0.8 , min_child_weight = 1, depth = 5, min_leaf = 5, learning_rate = 0.4, boosting_rounds = 5, lambda_ = 1.5, gamma = 1, eps = 0.1):\n",
    "        self.X, self.y = X, y\n",
    "        self.depth = depth\n",
    "        self.subsample_cols = subsample_cols\n",
    "        self.eps = eps\n",
    "        self.min_child_weight = min_child_weight \n",
    "        self.min_leaf = min_leaf\n",
    "        self.learning_rate = learning_rate\n",
    "        self.boosting_rounds = boosting_rounds \n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma  = gamma\n",
    "    \n",
    "        self.base_pred = np.full((X.shape[0], 1), np.mean(y)).flatten().astype('float64')\n",
    "    \n",
    "        for booster in range(self.boosting_rounds):\n",
    "            Grad = self.grad(self.base_pred, self.y)\n",
    "            Hess = self.hess(self.base_pred, self.y)\n",
    "            boosting_tree = XGBoostTree().fit(self.X, Grad, Hess, depth = self.depth, min_leaf = self.min_leaf, lambda_ = self.lambda_, gamma = self.gamma, eps = self.eps, min_child_weight = self.min_child_weight, subsample_cols = self.subsample_cols)\n",
    "            self.base_pred += self.learning_rate * boosting_tree.predict(self.X)\n",
    "            self.estimators.append(boosting_tree)\n",
    "          \n",
    "    def predict(self, X):\n",
    "        pred = np.zeros(X.shape[0])\n",
    "        \n",
    "        for estimator in self.estimators:\n",
    "            pred += self.learning_rate * estimator.predict(X) \n",
    "          \n",
    "        return np.full((X.shape[0], 1), np.mean(self.y)).flatten().astype('float64') + pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, X, y):\n",
    "        self.client_X = X\n",
    "        self.client_y = y\n",
    "        self.hist_data = {}\n",
    "        self.model = None\n",
    "        self.initial_preds = None\n",
    "\n",
    "\n",
    "    def compute_histograms(self, max_bins=256, global_bins=None):\n",
    "        X = self.client_X\n",
    "        y = self.client_y.to_numpy() if isinstance(self.client_y, pd.Series) else self.client_y\n",
    "        \n",
    "        if self.model:\n",
    "            y_preds = self.model.predict(X)\n",
    "        elif self.initial_preds is not None:\n",
    "            y_preds = self.initial_preds\n",
    "        else:\n",
    "            raise ValueError(\"No initial predictions available.\")\n",
    "\n",
    "        for column in X.columns:\n",
    "            # Compute quantile-based bins\n",
    "            if global_bins:\n",
    "                bin_edges = global_bins[column]\n",
    "            else:\n",
    "                bin_edges = np.quantile(X[column], q=np.linspace(0, 1, max_bins + 1))\n",
    "            \n",
    "            # Digitize feature values into bins\n",
    "            bin_indices = np.digitize(X[column], bins=bin_edges, right=False) - 1\n",
    "            bin_indices[bin_indices == max_bins] = max_bins - 1  # Handle edge case for right boundary\n",
    "            \n",
    "            # Preallocate arrays for gradients and Hessians\n",
    "            gradients = np.zeros(max_bins)\n",
    "            hessians = np.zeros(max_bins)\n",
    "            \n",
    "            # Aggregate gradients and Hessians for each bin\n",
    "            for bin_idx in range(max_bins):\n",
    "                indices = np.where(bin_indices == bin_idx)[0]\n",
    "                if len(indices) > 0:\n",
    "                    gradients[bin_idx] = np.sum(self.grad(y_preds[indices], y[indices]))\n",
    "                    hessians[bin_idx] = np.sum(self.hess(y_preds[indices], y[indices]))\n",
    "            \n",
    "            # Store results in the histogram data\n",
    "            self.hist_data[column] = {\n",
    "                'gradient': gradients,\n",
    "                'hessian': hessians,\n",
    "                'bin_edges': bin_edges\n",
    "            }\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # first order gradient logLoss\n",
    "    def grad(self, preds, labels):\n",
    "        preds = self.sigmoid(preds)\n",
    "        return(preds - labels)\n",
    "    \n",
    "    # second order gradient logLoss\n",
    "    def hess(self, preds, labels):\n",
    "        preds = self.sigmoid(preds)\n",
    "        return(preds * (1 - preds))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from customxgboost import XGBoostTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedXGBoost(XGBoostClassifier):\n",
    "    \n",
    "    def __init__(self, clients, max_bins=256):\n",
    "        super().__init__()\n",
    "        self.clients = clients\n",
    "        self.max_bins = max_bins\n",
    "        self.base_pred = None\n",
    "        self.global_bins = None\n",
    "        self.global_model = None\n",
    "        self.global_hist = None\n",
    "        self.global_y = np.array([])\n",
    "\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        ''' initialize base preds and global bins '''\n",
    "\n",
    "\n",
    "        for client in self.clients:\n",
    "            self.global_y = np.concatenate([self.global_y, client.client_y])\n",
    "\n",
    "        self.base_pred = self.base_predict()\n",
    "\n",
    "        for client in self.clients:\n",
    "            client.initial_preds = self.base_pred\n",
    "            client.compute_histograms(max_bins=self.max_bins)\n",
    "\n",
    "        # get global bins for each feature, accounting for intersecting bin edges\n",
    "        global_bins = {}\n",
    "        for client in self.clients:\n",
    "            for feature, data in client.hist_data.items():\n",
    "                if feature not in global_bins:\n",
    "                    global_bins[feature] = data['bin_edges']\n",
    "                else:\n",
    "                    global_bins[feature] = np.unique(np.concatenate([global_bins[feature], data['bin_edges']]))\n",
    "\n",
    "        # create global histogram\n",
    "        global_hist = {}\n",
    "        for feature, bins in global_bins.items():\n",
    "            global_hist[feature] = {\n",
    "                'gradient': np.zeros(len(bins) - 1),\n",
    "                'hessian': np.zeros(len(bins) - 1),\n",
    "                'bin_edges': bins\n",
    "            }\n",
    "\n",
    "        # aggregate gradients and Hessians, if global bins are smaller than client bins, fractionally distribute the gradients and Hessians, otherwise, sum them\n",
    "        for client in self.clients:\n",
    "            for feature, data in client.hist_data.items():\n",
    "                client_bins = data['bin_edges']\n",
    "                client_gradients = data['gradient']\n",
    "                client_hessians = data['hessian']\n",
    "\n",
    "                global_gradients = global_hist[feature]['gradient']\n",
    "                global_hessians = global_hist[feature]['hessian']\n",
    "                global_bins = global_hist[feature]['bin_edges']\n",
    "\n",
    "                # Step 3.1: Aggregate gradients and Hessians (client bins > global bins)\n",
    "                for i in range(len(global_bins) - 1):\n",
    "                    # Loop through global bins and aggregate from client bins\n",
    "                    global_bin_start = global_bins[i]\n",
    "                    global_bin_end = global_bins[i + 1]\n",
    "\n",
    "                    # Find the client bins that overlap with the global bin\n",
    "                    for j in range(len(client_bins) - 1):\n",
    "                        client_bin_start = client_bins[j]\n",
    "                        client_bin_end = client_bins[j + 1]\n",
    "\n",
    "                        # If the global bin overlaps with the client bin\n",
    "                        if (global_bin_start >= client_bin_start and global_bin_start < client_bin_end) or \\\n",
    "                        (global_bin_end > client_bin_start and global_bin_end <= client_bin_end):\n",
    "                            # Compute the overlapping length of the bins\n",
    "                            overlap_start = max(global_bin_start, client_bin_start)\n",
    "                            overlap_end = min(global_bin_end, client_bin_end)\n",
    "\n",
    "                            overlap_length = overlap_end - overlap_start\n",
    "                            client_bin_length = client_bin_end - client_bin_start\n",
    "                            proportion = overlap_length / client_bin_length\n",
    "\n",
    "                            # Calculate the contribution of the client bin to the global bin\n",
    "                            bin_idx = i  # Global bin index\n",
    "                            global_gradients[bin_idx] += client_gradients[j] * proportion\n",
    "                            global_hessians[bin_idx] += client_hessians[j] * proportion\n",
    "\n",
    "        # reduce the global bins to max_bins by aggregation according to quantile method\n",
    "        for feature, data in global_hist.items():\n",
    "            global_bins = data['bin_edges']\n",
    "            global_gradients = data['gradient']\n",
    "            global_hessians = data['hessian']\n",
    "\n",
    "            # Reduce the number of bins to max_bins\n",
    "            bin_edges = np.quantile(global_bins, q=np.linspace(0, 1, self.max_bins + 1))\n",
    "            new_gradients = np.zeros(self.max_bins)\n",
    "            new_hessians = np.zeros(self.max_bins)\n",
    "\n",
    "            for i in range(self.max_bins):\n",
    "                indices = np.where((global_bins >= bin_edges[i]) & (global_bins < bin_edges[i + 1]))[0]\n",
    "                if len(indices) > 0:\n",
    "                    new_gradients[i] = np.sum(global_gradients[indices])\n",
    "                    new_hessians[i] = np.sum(global_hessians[indices])\n",
    "\n",
    "            global_hist[feature] = {\n",
    "                'gradient': new_gradients,\n",
    "                'hessian': new_hessians,\n",
    "                'bin_edges': bin_edges\n",
    "            }\n",
    "\n",
    "        # store global bins and histogram\n",
    "        self.global_bins = {feature: data['bin_edges'] for feature, data in global_hist.items()}\n",
    "        self.global_hist = global_hist\n",
    "\n",
    "\n",
    "    def histogram_aggregation(self, initial_preds=None):\n",
    "        \"\"\"\n",
    "        Aggregate the histograms from all clients into a global histogram.\n",
    "        \"\"\"\n",
    "\n",
    "        for client in self.clients:\n",
    "            client.model = self.global_model\n",
    "            client.compute_histograms(max_bins=self.max_bins, global_bins=self.global_bins)\n",
    "\n",
    "        \n",
    "        # aggregate simply, since global bins are already defined and same for all clients\n",
    "        for client in self.clients:\n",
    "            for feature, data in client.hist_data.items():\n",
    "                client_gradients = data['gradient']\n",
    "                client_hessians = data['hessian']\n",
    "\n",
    "                global_gradients = self.global_hist[feature]['gradient']\n",
    "                global_hessians = self.global_hist[feature]['hessian']\n",
    "\n",
    "                global_gradients += client_gradients\n",
    "                global_hessians += client_hessians\n",
    "\n",
    "\n",
    "    def base_predict(self):\n",
    "        # calclate binary classification base_pred\n",
    "        P = np.mean(self.global_y)\n",
    "        base_pred = np.log(P / (1 - P))\n",
    "        self.base_y = base_pred \n",
    "        base_pred_array = np.full_like(self.global_y, base_pred)\n",
    "        return base_pred_array\n",
    "\n",
    "\n",
    "    \n",
    "    def fit(self, X, subsample_cols = 0.8 , min_child_weight = 1, depth = 5, min_leaf = 5, learning_rate = 0.4, boosting_rounds = 5, lambda_ = 1.5, gamma = 1, eps = 0.1):\n",
    "\n",
    "        self.depth = depth\n",
    "        self.subsample_cols = subsample_cols\n",
    "        self.eps = eps\n",
    "        self.min_child_weight = min_child_weight \n",
    "        self.min_leaf = min_leaf\n",
    "        self.learning_rate = learning_rate\n",
    "        self.boosting_rounds = boosting_rounds \n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma  = gamma\n",
    "    \n",
    "\n",
    "        self.max_bin = 256\n",
    "        self.global_model = self.base_pred\n",
    "        for booster in range(self.boosting_rounds):\n",
    "            boosting_tree = XGBoostTree().hist_fit(self.global_hist, subsample_cols = self.subsample_cols, min_leaf = self.min_leaf, min_child_weight = self.min_child_weight, depth = self.depth, lambda_ = self.lambda_, gamma = self.gamma, eps = self.eps)\n",
    "            self.global_model += self.learning_rate * boosting_tree.predict(X)\n",
    "            self.estimators.append(boosting_tree)\n",
    "            self.histogram_aggregation()\n",
    "\n",
    "        print('Training Complete')\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # pred is a 1D array of all values equal to self.base_y of shape (X.shape[0],): [y, y, y, y ...]\n",
    "        np.full((X.shape[0], 1), self.base_y).flatten().astype('float64')\n",
    "\n",
    "        for estimator in self.estimators:\n",
    "            pred += self.learning_rate * estimator.predict(X) \n",
    "          \n",
    "        return(self.sigmoid(np.full((X.shape[0], 1), 1).flatten().astype('float64') + pred))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.method == 'regular':\n",
    "            X = X.to_numpy() if isinstance(X, pd.DataFrame) else X\n",
    "        pred = np.full((X.shape[0], 1), self.base_y).flatten().astype('float64')\n",
    "        for estimator in self.estimators:\n",
    "            pred += self.learning_rate * estimator.predict(X) \n",
    "        \n",
    "        predicted_probas = self.sigmoid(np.full((X.shape[0], 1), 1).flatten().astype('float64') + pred)\n",
    "        preds = np.where(predicted_probas > np.mean(predicted_probas), 1, 0)\n",
    "        return(preds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot convert the series to <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m fed_model \u001b[38;5;241m=\u001b[39m FedXGBoost([client1, client2, client3])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# fit the model\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43mfed_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 161\u001b[0m, in \u001b[0;36mFedXGBoost.fit\u001b[1;34m(self, X, subsample_cols, min_child_weight, depth, min_leaf, learning_rate, boosting_rounds, lambda_, gamma, eps)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_pred\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m booster \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboosting_rounds):\n\u001b[1;32m--> 161\u001b[0m     boosting_tree \u001b[38;5;241m=\u001b[39m \u001b[43mXGBoostTree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhist_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_hist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubsample_cols\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubsample_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_leaf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_leaf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_child_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_child_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlambda_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_model \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m boosting_tree\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators\u001b[38;5;241m.\u001b[39mappend(boosting_tree)\n",
      "Cell \u001b[1;32mIn[26], line 371\u001b[0m, in \u001b[0;36mXGBoostTree.hist_fit\u001b[1;34m(self, histograms, subsample_cols, min_leaf, min_child_weight, depth, lambda_, gamma, eps)\u001b[0m\n\u001b[0;32m    369\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(histograms\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m    370\u001b[0m num_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(feature_names)\n\u001b[1;32m--> 371\u001b[0m num_selected_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubsample_cols\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    372\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(feature_names, num_selected_features, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# Filter histograms to only include selected features\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\series.py:248\u001b[0m, in \u001b[0;36m_coerce_method.<locals>.wrapper\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    240\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconverter\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on a single element Series is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will raise a TypeError in the future. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    246\u001b[0m     )\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m converter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m--> 248\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot convert the series to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconverter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot convert the series to <class 'int'>"
     ]
    }
   ],
   "source": [
    "# make data of 1000 rows and 5 features\n",
    "data = create_dummy_data(n_samples=1000, n_features=5)\n",
    "\n",
    "# seperate 20 percent of total data for testing\n",
    "X = data.drop('label', axis=1)\n",
    "y = data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# create 3 clients\n",
    "client1 = Client(X_train[:300], y_train[:300])\n",
    "client2 = Client(X_train[300:600], y_train[300:600])\n",
    "client3 = Client(X_train[600:], y_train[600:])\n",
    "\n",
    "# create a federated model with the 3 clients\n",
    "fed_model = FedXGBoost([client1, client2, client3])\n",
    "\n",
    "# fit the model\n",
    "fed_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
